{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n",
      "Kartrocket\n",
      "2008\n",
      "ACS Global Tech Solutions Pvt Ltd\n",
      "ACS Global Tech Solutions   Ltd\n",
      "ACS Global Tech Solutions    \n",
      "2009\n",
      "BrickRed Technologies\n",
      "2010\n",
      "emPhi\n",
      "2011\n",
      "IISGL\n",
      "2012\n",
      "Appinventiv\n",
      "2013\n",
      "Shriv Commedia Solutions Pvt Ltd\n",
      "Shriv Commedia Solutions   Ltd\n",
      "Shriv Commedia Solutions    \n",
      "2014\n",
      "CodeIgnition\n",
      "2015\n",
      "Yapapaya Technologies LLP\n",
      "2016\n",
      "Astute Systems Technology Private Limited\n",
      "Astute Systems Technology   Limited\n",
      "Astute Systems Technology    \n",
      "2017\n",
      "S-EDGE Software\n",
      "2018\n",
      "Cross Byrne Design Studio Pvt. Ltd.\n",
      "Cross Byrne Design Studio   Ltd.\n",
      "Cross Byrne Design Studio    \n",
      "2019\n",
      "HappyShappy\n",
      "2020\n",
      "IndyLogix Solutions\n",
      "2021\n",
      "Stigasoft\n",
      "2022\n",
      "Admiral Technologies\n",
      "2023\n",
      "Bhramm Technologies\n",
      "2024\n",
      "Weaverbird Engineering & Technology Pvt Ltd\n",
      "Weaverbird Engineering & Technology   Ltd\n",
      "Weaverbird Engineering & Technology    \n",
      "2025\n",
      "M1 LOGIX TECHNOLOGIES\n",
      "2026\n",
      "jusTap\n",
      "2027\n",
      "Depasser Infotech\n",
      "2028\n",
      "Future Next Technologies\n",
      "2029\n",
      "Ariel Software Solutions Pvt. Ltd.\n",
      "Ariel Software Solutions   Ltd.\n",
      "Ariel Software Solutions    \n",
      "2030\n",
      "Sterco Digitex Pvt Limited\n",
      "Sterco Digitex Pvt  \n",
      "Sterco Digitex    \n",
      "2031\n",
      "Avishkar Soft Labs Pvt Ltd\n",
      "Avishkar Soft Labs   Ltd\n",
      "Avishkar Soft Labs    \n",
      "2032\n",
      "TVSi\n",
      "2033\n",
      "iLabs\n",
      "2034\n",
      "Milletfield Technologies\n",
      "2035\n",
      "Key Concepts IT services LLP\n",
      "2036\n",
      "Aletheia Technologies Pvt.Ltd\n",
      "Aletheia Technologies  Ltd\n",
      "Aletheia Technologies   \n",
      "2037\n",
      "Benelec Infotech\n",
      "2038\n",
      "Covalent Softwares Pvt. Ltd.\n",
      "Covalent Softwares   Ltd.\n",
      "Covalent Softwares    \n",
      "2039\n",
      "KCP Technologies Limited\n",
      "KCP Technologies  \n",
      "2040\n",
      "Neuro SEO Services\n",
      "2041\n",
      "GrandTrust Infotech (P) Ltd\n",
      "GrandTrust Infotech (P)  \n",
      "2042\n",
      "SpearheadIT Pvt Ltd\n",
      "SpearheadIT   Ltd\n",
      "SpearheadIT    \n",
      "2043\n",
      "Pyther Innovations Private Limited\n",
      "Pyther Innovations   Limited\n",
      "Pyther Innovations    \n",
      "2044\n",
      "ePeople Bespoke Consulting Pvt Ltd\n",
      "ePeople Bespoke Consulting   Ltd\n",
      "ePeople Bespoke Consulting    \n",
      "2045\n",
      "Sapple Systems\n",
      "2046\n",
      "Pen Groups\n",
      "2047\n",
      "AxisTechnolabs\n",
      "2048\n",
      "Salmon Commerce Pvt Ltd\n",
      "Salmon Commerce   Ltd\n",
      "Salmon Commerce    \n",
      "2049\n",
      "Lakhshya Cyber Security Labs Pvt Ltd\n",
      "Lakhshya Cyber Security Labs   Ltd\n",
      "Lakhshya Cyber Security Labs    \n",
      "2050\n",
      "Imaginet Ventures Pvt Ltd, India\n",
      "Imaginet Ventures   Ltd, India\n",
      "Imaginet Ventures    , India\n",
      "2051\n",
      "Elixir Web Solutions\n",
      "2052\n",
      "Samrudh Solutions\n",
      "2053\n",
      "Codifier Technologies Pvt. Ltd.\n",
      "Codifier Technologies   Ltd.\n",
      "Codifier Technologies    \n",
      "2054\n",
      "EYT Ltd.\n",
      "EYT  \n",
      "2055\n",
      "S4 IT Technologies\n",
      "2056\n",
      "Computer Ware India Private Limited\n",
      "Computer Ware India   Limited\n",
      "Computer Ware India    \n",
      "2057\n",
      "Micronesoft\n",
      "2058\n",
      "NRK DIGITAL\n",
      "2059\n",
      "Mavin Apps and Technologies private limited\n",
      "2060\n",
      "Vista Info Systems\n",
      "2061\n",
      "NexBorg Software Solutions Private Limited\n",
      "NexBorg Software Solutions   Limited\n",
      "NexBorg Software Solutions    \n",
      "2062\n",
      "Radianz Services\n",
      "2063\n",
      "Seegesys\n",
      "2064\n",
      "UBR Online Solutions\n",
      "2065\n",
      "Aptegra Solutions Private Limited\n",
      "Aptegra Solutions   Limited\n",
      "Aptegra Solutions    \n",
      "2066\n",
      "Panzer Technologies Pvt Ltd\n",
      "Panzer Technologies   Ltd\n",
      "Panzer Technologies    \n",
      "2067\n",
      "CodingBrains Software Solutions Pvt. Ltd\n",
      "CodingBrains Software Solutions   Ltd\n",
      "CodingBrains Software Solutions    \n",
      "2068\n",
      "Daboya Technologies Pvt Ltd. (Formerly Known as Jiffy Software Solutions)\n",
      "Daboya Technologies Pvt   (Formerly Known as Jiffy Software Solutions)\n",
      "Daboya Technologies     (Formerly Known as Jiffy Software Solutions)\n",
      "2069\n",
      "The One Technologies\n",
      "2070\n",
      "Coherent Lab LLP\n",
      "2071\n",
      "Digimason Web Pvt. Ltd\n",
      "Digimason Web   Ltd\n",
      "Digimason Web    \n",
      "2072\n",
      "Zingbytes IT Solution\n",
      "2073\n",
      "iGauri Solutions Pvt. Ltd\n",
      "iGauri Solutions   Ltd\n",
      "iGauri Solutions    \n",
      "2074\n",
      "Sanmok Solutions Pvt. Ltd.\n",
      "Sanmok Solutions   Ltd.\n",
      "Sanmok Solutions    \n",
      "2075\n",
      "Sharma Infoway\n",
      "2076\n",
      "iMuhandis Solutions Private Limited\n",
      "iMuhandis Solutions   Limited\n",
      "iMuhandis Solutions    \n",
      "2077\n",
      "Wings Tech Solutions\n",
      "2078\n",
      "Global Mobile App Summit & Awards (GMASA)\n",
      "2079\n",
      "Radikal Labs - Web, Game & Mobile App Development Company\n",
      "2080\n",
      "TechnoCube ( A Mobile App Development Company)\n",
      "2081\n",
      "YDoodle - Web & Mobile App Development Company\n",
      "2082\n",
      "Evince Mobile App Development\n",
      "2083\n",
      "Abacus Desk\n",
      "2084\n",
      "Crofarm\n",
      "2085\n",
      "iMerit Technology\n",
      "2086\n",
      "Futurenet Krishna Softech\n",
      "2087\n",
      "Accord Software & Systems\n",
      "2088\n",
      "Aricon Software LLP\n",
      "2089\n",
      "GHCL\n",
      "2090\n",
      "WebCraft IT\n",
      "2091\n",
      "Ambrosia\n",
      "2092\n",
      "Fourth Ambit\n",
      "2093\n",
      "2Base Technologies Pvt. Ltd.\n",
      "2Base Technologies   Ltd.\n",
      "2Base Technologies    \n",
      "2094\n",
      "Talent Recruit Software Pvt. Ltd.\n",
      "Talent Recruit Software   Ltd.\n",
      "Talent Recruit Software    \n",
      "2095\n",
      "Wegilant\n",
      "2096\n",
      "KRISH IT SOLUTIONS PVT LTD\n",
      "2097\n",
      "Amstar Technologies Pvt Ltd\n",
      "Amstar Technologies   Ltd\n",
      "Amstar Technologies    \n",
      "2098\n",
      "NZT Solutions Pvt. ltd.\n",
      "NZT Solutions   ltd.\n",
      "2099\n",
      "Talent Canvas- HR, Writing, Designing and Advertising Solutions\n",
      "2100\n",
      "DataCede™Chennai Private Limited\n",
      "DataCede™Chennai   Limited\n",
      "DataCede™Chennai    \n",
      "2101\n",
      "WizWyz Technologies\n",
      "2102\n",
      "Webart Softech\n",
      "2103\n",
      "Clover Infotech\n",
      "2104\n",
      "Legitwise Technologies\n",
      "2105\n",
      "Avon Technologies (I) Private Ltd.\n",
      "Avon Technologies (I) Private  \n",
      "Avon Technologies (I)    \n",
      "2106\n",
      "Careator Technologies\n",
      "2107\n",
      "ICAN Technology\n",
      "2108\n",
      "Quzera Web Technologies Private Limited\n",
      "Quzera Web Technologies   Limited\n",
      "Quzera Web Technologies    \n",
      "2109\n",
      "Mindnotix Technologies\n",
      "2110\n",
      "DEEPROOTS TECHNOLOGIES\n",
      "2111\n",
      "OASYS Cybernetics Pvt. Ltd\n",
      "OASYS Cybernetics   Ltd\n",
      "OASYS Cybernetics    \n",
      "2112\n",
      "Amulya Infotech India Pvt Ltd\n",
      "Amulya Infotech India   Ltd\n",
      "Amulya Infotech India    \n",
      "2113\n",
      "Webarch Club\n",
      "2114\n",
      "Popular Infotech Pvt Ltd\n",
      "Popular Infotech   Ltd\n",
      "Popular Infotech    \n",
      "2115\n",
      "GENX SOFT TECHNOLOGIES (P) LTD\n",
      "2116\n",
      "Avenues India Private Limited\n",
      "Avenues India   Limited\n",
      "Avenues India    \n",
      "2117\n",
      "InheritX Solutions PVT. LTD\n",
      "2118\n",
      "Planetasia\n",
      "2119\n",
      "VTaurus Technologies Private Limited\n",
      "VTaurus Technologies   Limited\n",
      "VTaurus Technologies    \n",
      "2120\n",
      "IIT Hyderabad\n",
      "2121\n",
      "4i Apps Solutions | Cloud Solutions Provider\n",
      "2122\n",
      "Eagle Software India Pvt Ltd.\n",
      "Eagle Software India Pvt  \n",
      "Eagle Software India    \n",
      "2123\n",
      "TagWings Technologies Pvt. Ltd.\n",
      "TagWings Technologies   Ltd.\n",
      "TagWings Technologies    \n",
      "2124\n",
      "Nucsoft\n",
      "2125\n",
      "SEOPhalanx\n"
     ]
    }
   ],
   "source": [
    "class Scrapper():\n",
    "    def __init__(self):\n",
    "        self.aka = 0\n",
    "        \n",
    "# This funtion will put the company name in the search field and press ENTER.\n",
    "# After this it'll search for the comapny name in the list appeared\n",
    "    def go_to_site(self, company_name):\n",
    "        \n",
    "        # Put company name in search firld and press ENTER\n",
    "        search = browser.find_element_by_name(\"q\")\n",
    "        try :\n",
    "            search.send_keys(company_name)\n",
    "        except WebDriverException as exception:\n",
    "            pass\n",
    "    \n",
    "        search.send_keys(Keys.ENTER)\n",
    "        \n",
    "        # Give it some time to load the list of related information\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Select the company name matching to the one we are searching for\n",
    "        related_info = browser.find_elements_by_class_name(\"title\")\n",
    "        for name in related_info:    \n",
    "            try:\n",
    "                if company_name.lower() in name.text.lower():\n",
    "                    name.click()\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "                \n",
    "   \n",
    "   # This funtion'll extract the required info like \"Location\",'Specialities','Company size','Website' \n",
    "    # from web page and save it into dictionary             \n",
    "    def get_company_info(self, c_info):\n",
    "        col = [\"Location\",'Specialities','Company size','Website']\n",
    "        info = {}\n",
    "        a = c_info.split('\\n')\n",
    "    \n",
    "        for i in range(len(a)):\n",
    "            info[col[i]] = a[i]\n",
    "        \n",
    "        # It is capturing Domain name from website\n",
    "        if \"Website\" in info.keys(): \n",
    "            for i in (info[\"Website\"]).split('.'):\n",
    "                if len(i) > 5 :\n",
    "                    info[\"Domain_name\"] = i\n",
    "        else:\n",
    "            pass\n",
    "        return info\n",
    "                \n",
    "    # This will extract name of all the investors investing in the company    \n",
    "    def get_investors(self,Investors):        \n",
    "        investors = {}        \n",
    "        l = Investors.split(\"\\n\")\n",
    "        inv = [\"Inverstor_1\", \"Inverstor_2\",\"Inverstor_3\",\"Inverstor_4\", \"Inverstor_5\",\"Inverstor_6\"]\n",
    "        k = 0\n",
    "        \n",
    "        # To avoid extracting their professions, select words with \n",
    "        # length greater than or equal to 2 and less than or equal to 3\n",
    "        for i in range(len(l)):\n",
    "            if len(l[i].split(\" \")) >= 2:\n",
    "                j = l[i].split(\" \")\n",
    "\n",
    "                if len(j[0]) > 3 and len(j[1]) > 3:\n",
    "                    investors[inv[k]] = l[i]\n",
    "                    k =+ 1\n",
    "                else:\n",
    "                    pass\n",
    "        return investors\n",
    "    \n",
    "    # This will get info of all the founders of the company\n",
    "    def get_founders(self, founders):\n",
    "        founder = {}\n",
    "        f = founders.split('\\n') \n",
    "        fdr = []\n",
    "        for i in range(len(f)):\n",
    "            if len(f[i].split(' ')) >= 2 and len(f[i].split(' ')) <= 3:\n",
    "                fdr.append(f[i])\n",
    "            else:\n",
    "                pass\n",
    "        fdr_list = [\"Founder_1\", \"Founder_2\", \"Founder_3\", \"Founder_4\", \"Founder_5\",\"Founder_6\", \"Founder_7\", \"Founder_8\", \"Founder_9\", \"Founder_10\"]\n",
    "\n",
    "        for i in range(len(fdr)):\n",
    "            founder[fdr_list[i]] = fdr[i]\n",
    "        return founder\n",
    "    \n",
    "    # This will extract sum of all the fundings the company is receiving\n",
    "    def get_fundings(self, funds):\n",
    "        value = 0\n",
    "        for i in funds:\n",
    "            try : \n",
    "                j = int(i.text.replace(',', '')[1:])\n",
    "                value = value + j\n",
    "            except ValueError as exception:\n",
    "                pass\n",
    "        return value\n",
    "    \n",
    "    # This funtion will save data into csv file\n",
    "     def dict_to_csv(self, scraped_values_dict): \n",
    "        my_dict = scraped_values_dict\n",
    "        with open('Data_1.csv', 'a') as f:\n",
    "            w = csv.DictWriter(f, col_c_info)\n",
    "            if f.tell() == 0:\n",
    "                w.writeheader()\n",
    "                w.writerow(my_dict)\n",
    "            else: \n",
    "                w.writerow(my_dict)\n",
    "                \n",
    "                \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import getpass\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, WebDriverException\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "chromedriver = '/usr/bin/chromedriver'\n",
    "browser = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# Open the site\n",
    "browser.get('https://angel.co/login')\n",
    "\n",
    "# Provide username and password\n",
    "username = browser.find_element_by_id(\"user_email\")\n",
    "password = browser.find_element_by_id(\"user_password\")\n",
    "\n",
    "username.send_keys(\"akankshakaple007@gmail.com\")\n",
    "password.send_keys(\"@k@nksh@007\")\n",
    "\n",
    "browser.find_element_by_class_name(\"c-button\").click()\n",
    "\n",
    "# Provide the list of compaanies to extract the info\n",
    "data = pd.read_csv(\"Data_with_result.csv\")\n",
    "company_name = np.array(data[\"Company_Name\"])\n",
    "\n",
    "# Elements we want from the website into csv file\n",
    "col_c_info = [\"Company_name\",'Website',\"Domain_name\",\"Location\",'Specialities','Company size'\n",
    "              ,\"LinkedIn_URL\",\"Funding\" ,\"Founder_1\", \"Founder_2\", \"Founder_3\", \"Founder_4\", \"Founder_5\"\n",
    "              ,\"Founder_6\",\"Founder_7\", \"Founder_8\", \"Founder_9\", \"Founder_10\"\n",
    "              ,\"Inverstor_1\", \"Inverstor_2\",\"Inverstor_3\",\"Inverstor_4\", \"Inverstor_5\",\"Inverstor_6\"\n",
    "              ,\"Job_1\", \"Job_2\", \"Job_3\", \"Job_4\", \"Job_5\", \"Job_6\", \"Job_7\", \"Job_8\", \"Job_9\",\"Job_10\"]\n",
    "\n",
    "\n",
    "# Create an object for class Scraper\n",
    "s = Scrapper()\n",
    "\n",
    "for i in range(2007, 2300):\n",
    "    print(i)  # Print value for verification\n",
    "    \n",
    "    c_info = {}\n",
    "    investors = {}\n",
    "    founders = {}\n",
    "    dict_1 = {}\n",
    "    \n",
    "    print(company_name[i]) # Print current company name for verification\n",
    "    \n",
    "    # As company name might contain some extra words, so remove them\n",
    "    cn = data[\"Company_Name\"][i]\n",
    "    l = ['Pvt.', 'Ltd.', \"Private\", \"Limited\", \"Pvt\", \"Ltd\"]\n",
    "    for j in range(len(l)):\n",
    "        if l[j] in cn:\n",
    "            cn = cn.replace(l[j], ' ')\n",
    "            print(cn)\n",
    "        \n",
    "    dict_1[\"Company_name\"] = company_name[i]\n",
    "    \n",
    "    try :\n",
    "        s.go_to_site(cn)\n",
    "        \n",
    "    except StaleElementReferenceException as exception:\n",
    "        pass\n",
    "    except TimeoutException as exception:\n",
    "        pass\n",
    "    except WebDriverException as exception:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    try :     \n",
    "        c_info = s.get_company_info(browser.find_element_by_class_name('section_container').text)\n",
    "        \n",
    "    except NoSuchElementException as exception:\n",
    "        try :\n",
    "            s.go_to_site(cn)\n",
    "            time.sleep(5)\n",
    "            c_info = s.get_company_info(browser.find_element_by_class_name('section_container').text)\n",
    "            \n",
    "        except NoSuchElementException as exception:\n",
    "            pass\n",
    "    \n",
    "    try :\n",
    "        linkedin_url = browser.find_element_by_class_name('linkedin_url').get_attribute('href')\n",
    "        dict_1[\"LinkedIn_URL\"] = linkedin_url\n",
    "    except NoSuchElementException as exception:\n",
    "        pass\n",
    "    \n",
    "    try : \n",
    "        founders = s.get_founders(browser.find_element_by_class_name('startup_profile_group').text)\n",
    "    except NoSuchElementException as exception:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        investors = s.get_investors(browser.find_element_by_class_name('medium').text)\n",
    "    except NoSuchElementException as exception:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        dict_1[\"Funding\"] = s.get_fundings(browser.find_elements_by_class_name(\"raised\"))\n",
    "    except NoSuchElementException as exception:\n",
    "        pass\n",
    "    \n",
    "    # Go to the company's job application page to get all the jobs company is offering \n",
    "    # related to software\n",
    "    try :\n",
    "        browser.find_element_by_class_name(\"u-widthFull\").click()\n",
    "    except NoSuchElementException as exception:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "\n",
    "    jobs = {}\n",
    "       \n",
    "    k = 0\n",
    "    j = []   \n",
    "    \n",
    "    # This class name differs from company to company \n",
    "    try : \n",
    "\n",
    "        jobs_avail = browser.find_element_by_class_name(\"s-grid-colMd18\").text\n",
    "        j_list = jobs_avail.split(\"\\n\")\n",
    "        k = 0\n",
    "        j = [\"Job_1\", \"Job_2\", \"Job_3\", \"Job_4\", \"Job_5\", \"Job_6\", \"Job_7\", \"Job_8\", \"Job_9\",\"Job_10\"]\n",
    "        l = j_list[::3]\n",
    "        l = l[:10]\n",
    "        for i in l:\n",
    "            jobs[j[k]] = i\n",
    "            k = k + 1\n",
    "        dict_1 = {**dict_1, **jobs}\n",
    "        \n",
    "    except NoSuchElementException as exception:\n",
    "        \n",
    "        try :\n",
    "            jobs_avail = browser.find_element_by_class_name(\"listing-title\").text\n",
    "            j_list = jobs_avail.split(\"\\n\")\n",
    "            k = 0\n",
    "            j = [\"Job_1\", \"Job_2\", \"Job_3\", \"Job_4\", \"Job_5\", \"Job_6\", \"Job_7\", \"Job_8\", \"Job_9\",\"Job_10\"]\n",
    "            l = j_list[::3]\n",
    "            l = l[:10]\n",
    "            for i in l:\n",
    "                jobs[j[k]] = i\n",
    "                k = k + 1\n",
    "            dict_1 = {**dict_1, **jobs}\n",
    "            \n",
    "        except NoSuchElementException as exception:\n",
    "            pass\n",
    "        \n",
    "    # After extracting info of jobs, go back to the home page    \n",
    "    browser.back()     \n",
    "    \n",
    "    # Combine all the info into a dictionary\n",
    "    my_dict = {**c_info, **investors, **founders, **dict_1}   \n",
    "           \n",
    "    # Convert dictionary into csv file\n",
    "    s.dict_to_csv(my_dict)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
